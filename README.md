# LiSpiderLiSpider is a lightweight python module that is used to ease the effort of scraping/extracting HTML elements. By just specifying some of your customized Template Varible as the HTML elements attributes/texts, then all of them presents to you as a well-wrapped python dict object.## InstallLiSpider has not been uploaded to PyPI currently, but you can download the wheel package [lispider-0.1.dev0.tar.gz](https://github.com/jay7n/LiSpider/blob/master/dist/lispider-0.1.dev0.tar.gz) (click _'View Raw'_ in case you have no idea how to), and install it using your favorite pip.```    pip install lispider-0.1.dev0.tar.gz```## Usage### Config Your Config file first* #### Specify the urls of web pages which your're interested in scraping    You need to specify which one (or which ones) web page(s) you want to scrape.      For example you're going to take some image samples from [xkcd.com](xkcd.com). According to the assumption of your knowledge to this site, you already know each page has been assigned to an well encoded url, most of which look like this :    > http://xkcd.com/706/      > http://xkcd.com/707/      > http://xkcd.com/708/      > ......    And each page has been filled up in with an image telling a funny yet great story. Your purpose is just to extract these image links (which is of course a html element). So first you need to tell the spider where you want to pick them up.    ``` pythonGrabHtmlContent = {    'URLScope': [        'http://xkcd.com/706',        'http://xkcd.com/707',        'http://xkcd.com/708',    ],    'MaxTryCount': 5}    ```    The key __URLScope__ in __GrabHtmlContent__ corresponds to a list where you can put as many as you can url items. The spider will loop for this list to get the interested html elements.    __MaxTryCount__ means that sometimes there're some glitches in opening the url link and then how many times you would like to let the spider try once again. In this example we give it 5 times to try each page.    Often we have a sense that in fact the pages we're taking are aligned by orders. For example here they're 706-708, so it's not necessary to specify each ordered page one by one actually. You can make things easy by using __Template Specifier "%"__ (The idea is inspired by _Jinja2_):    ```pythonGrabHtmlContent = {    'URLScope': [        'http://xkcd.com/%706-708%/',        'http://xkcd.com/100/', # this gives you the idea that you can combine both styles.    ],    'MaxTryCount': 5}    ```* #### Specify the html template that fits your interests    Also you need to make it clear which specific one (or ones) html element(s) in the web page you'd like to extract.    ```    ```* #### Specify others if you'd like to take care    There're some other options you can set, and you can safely ignore them if you trust the default settings for them.    ```    ```So basically after all these configs get done, the config file looks like this:``````### Run LiSpider with Your Config fileWith the config file prepared well, you need to import the spider module and the config file first.Feed the spider with the config and activate it by calling its '_Run()_' method, then the spider will orgnize all your interests as a python dict object and return it to you.``` pythonfrom lispider import Spiderimport your_config as configspider = Spider(config)results = spider.Run()print(results)```In the above case's circumstance you're supposed to see the results as this:``````## Demos* ### ImgDownloader* ### RssChef## What's Next1. Use json instead of python script itself as a more flexible config format* Add a multithread-worker to improve the scraping performance.* Add a js-evaluater to parse the javascript embedded in the html elements so as to get the final true html elements.